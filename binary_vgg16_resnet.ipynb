{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43569347",
   "metadata": {},
   "source": [
    "# Models ResNet and VGG16 for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15ed82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global imports for vgg16\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import skimage.io\n",
    "import os \n",
    "import tqdm\n",
    "import glob\n",
    "import tensorflow \n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.color import grey2rgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPool2D, Conv2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "#import tensorflow_addons as tfa\n",
    "#from tensorflow.keras.metrics import Metric\n",
    "#from tensorflow_addons.utils.types import AcceptableDTypes, FloatTensorLike\n",
    "from typeguard import typechecked\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7b511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for dataset generation - autotune\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# generating train/test/valid\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   validation_split = 0.2,\n",
    "                                  \n",
    "        rotation_range=5,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  validation_split = 0.2)\n",
    "\n",
    "test_datagen  = ImageDataGenerator(rescale = 1./255\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "752aded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 567 images belonging to 2 classes.\n",
      "Found 25 images belonging to 2 classes.\n",
      "Found 68 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# with vgg16 models the target size is always 224x224\n",
    "train_dataset  = train_datagen.flow_from_directory(directory = './Desktop/CTscns2C/train',\n",
    "                                                   target_size = (224,224),\n",
    "                                                   class_mode = 'categorical',\n",
    "                                                   batch_size = 64)\n",
    "\n",
    "# validation data\n",
    "valid_dataset = valid_datagen.flow_from_directory(directory = './Desktop/CTscns2C/val',\n",
    "                                                  target_size = (224,224),\n",
    "                                                  class_mode = 'categorical',\n",
    "                                                  batch_size = 64)\n",
    "\n",
    "# test data\n",
    "test_dataset = test_datagen.flow_from_directory(directory = './Desktop/CTscns2C/test',\n",
    "                                                  target_size = (224,224),\n",
    "                                                  class_mode = 'categorical',\n",
    "                                                  batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb649b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 15:33:23.600913: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# loading in the same vgg model used for the other data set\n",
    "\n",
    "base_model_v2 = tf.keras.applications.VGG16(input_shape=(224,224,3),include_top=False,weights=\"imagenet\")\n",
    "# using the weights used in the imagenet competition\n",
    "\n",
    "# Freezing layers\n",
    "\n",
    "for layer in base_model_v2.layers[:-8]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe5bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding sequential layers/ building model - just as in the first iteration\n",
    "\n",
    "model=Sequential()\n",
    "model.add(base_model_v2)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32,kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32,kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32,kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2,activation='softmax')) # must use softmax at last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a590b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defing f1 score using predictions\n",
    "\n",
    "def f1_score(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "METRICS = [\n",
    "      'accuracy',f1_score\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "737c299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 8.82 µs\n",
      "Epoch 1/40\n",
      "9/9 [==============================] - 85s 9s/step - loss: 0.8131 - accuracy: 0.5397 - f1_score: 0.5398 - val_loss: 10.7476 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "9/9 [==============================] - 82s 9s/step - loss: 0.7920 - accuracy: 0.5397 - f1_score: 0.5395 - val_loss: 10.3482 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "9/9 [==============================] - 82s 9s/step - loss: 0.6649 - accuracy: 0.5891 - f1_score: 0.5901 - val_loss: 5.0458 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "9/9 [==============================] - 85s 9s/step - loss: 0.5636 - accuracy: 0.7478 - f1_score: 0.7455 - val_loss: 2.5379 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "9/9 [==============================] - 83s 9s/step - loss: 0.4749 - accuracy: 0.8307 - f1_score: 0.8313 - val_loss: 1.9695 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "9/9 [==============================] - 83s 9s/step - loss: 0.4317 - accuracy: 0.8430 - f1_score: 0.8438 - val_loss: 1.3778 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 7/40\n",
      "9/9 [==============================] - 84s 9s/step - loss: 0.3966 - accuracy: 0.8765 - f1_score: 0.8773 - val_loss: 0.4964 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "9/9 [==============================] - 85s 9s/step - loss: 0.3551 - accuracy: 0.8942 - f1_score: 0.8947 - val_loss: 0.7111 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "9/9 [==============================] - 85s 9s/step - loss: 0.3122 - accuracy: 0.9189 - f1_score: 0.9182 - val_loss: 0.4488 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 10/40\n",
      "9/9 [==============================] - 84s 9s/step - loss: 0.2928 - accuracy: 0.9153 - f1_score: 0.9155 - val_loss: 5.3829 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 11/40\n",
      "9/9 [==============================] - 84s 9s/step - loss: 0.2796 - accuracy: 0.9242 - f1_score: 0.9248 - val_loss: 0.1108 - val_accuracy: 0.9600 - val_f1_score: 0.9600 - lr: 0.0010\n",
      "Epoch 12/40\n",
      "9/9 [==============================] - 85s 9s/step - loss: 0.2411 - accuracy: 0.9453 - f1_score: 0.9453 - val_loss: 0.4577 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 13/40\n",
      "9/9 [==============================] - 84s 9s/step - loss: 0.2335 - accuracy: 0.9506 - f1_score: 0.9508 - val_loss: 0.0228 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - lr: 0.0010\n",
      "Epoch 14/40\n",
      "9/9 [==============================] - 86s 10s/step - loss: 0.2273 - accuracy: 0.9312 - f1_score: 0.9314 - val_loss: 0.2983 - val_accuracy: 0.9200 - val_f1_score: 0.9200 - lr: 0.0010\n",
      "Epoch 15/40\n",
      "9/9 [==============================] - 83s 9s/step - loss: 0.2076 - accuracy: 0.9418 - f1_score: 0.9413 - val_loss: 0.0176 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - lr: 0.0010\n",
      "Epoch 16/40\n",
      "9/9 [==============================] - 83s 9s/step - loss: 0.1905 - accuracy: 0.9471 - f1_score: 0.9462 - val_loss: 0.1308 - val_accuracy: 0.9600 - val_f1_score: 0.9600 - lr: 0.0010\n",
      "Epoch 17/40\n",
      "9/9 [==============================] - 83s 9s/step - loss: 0.1894 - accuracy: 0.9506 - f1_score: 0.9505 - val_loss: 0.0149 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - lr: 0.0010\n",
      "Epoch 18/40\n",
      "9/9 [==============================] - 83s 9s/step - loss: 0.1589 - accuracy: 0.9700 - f1_score: 0.9705 - val_loss: 0.0854 - val_accuracy: 0.9600 - val_f1_score: 0.9600 - lr: 0.0010\n",
      "Epoch 19/40\n",
      "9/9 [==============================] - 83s 9s/step - loss: 0.1378 - accuracy: 0.9665 - f1_score: 0.9667 - val_loss: 0.1479 - val_accuracy: 0.9600 - val_f1_score: 0.9600 - lr: 0.0010\n",
      "Epoch 20/40\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.1394 - accuracy: 0.9665 - f1_score: 0.9667\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "9/9 [==============================] - 84s 9s/step - loss: 0.1394 - accuracy: 0.9665 - f1_score: 0.9667 - val_loss: 0.1614 - val_accuracy: 0.9600 - val_f1_score: 0.9600 - lr: 0.0010\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "# compile the model and run over ttraining data\n",
    "\n",
    "# adding early stopping and other call backs to avoid massive training times\n",
    "reduce = ReduceLROnPlateau(monitor = 'val_loss',patience = 3,verbose = 1,factor = 0.50, min_lr = 1e-7)# reducing learning rate, stopping overfitting\n",
    "\n",
    "early_stopping = EarlyStopping(verbose=1, patience=3)\n",
    "\n",
    "\n",
    "# compiling the model using adam as opt. and using x entropy for loss\n",
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = METRICS)\n",
    "\n",
    "# training the model over the training data, using validation data also\n",
    "%time\n",
    "history=model.fit(train_dataset,validation_data=valid_dataset,epochs = 40,verbose = 1, callbacks=[reduce, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "336ba256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 14:14:02.236652: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/elliotttrott/Desktop/Diss_saved_models/binarymodels/vgg16binary/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "\n",
    "model.save(\"./Desktop/Diss_saved_models/binarymodels/vgg16binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29bbb8f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 42s 5s/step - loss: 0.1120 - accuracy: 0.9700 - f1_score: 0.9699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1120179146528244, 0.9700176119804382, 0.9699178338050842]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate over the test data\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ac829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eaaa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85779266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8712c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports for resnet model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f97257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best results from designing generator with some augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True, \n",
    "    validation_split=0.2, \n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input,\n",
    "    dtype=tf.float32,\n",
    "    rotation_range=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74749995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 567 images belonging to 2 classes.\n",
      "Found 25 images belonging to 2 classes.\n",
      "Found 68 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# paths for training - setting class mode to sparse is the only difference with how these sets are compiled\n",
    "train_datasetRes  = train_datagen.flow_from_directory(directory = './Desktop/CTscns2C/train',\n",
    "                                                   target_size = (224,224),\n",
    "                                                   class_mode = 'sparse',\n",
    "                                                   batch_size = 64)\n",
    "\n",
    "# validation data\n",
    "valid_datasetRes = valid_datagen.flow_from_directory(directory = './Desktop/CTscns2C/val',\n",
    "                                                  target_size = (224,224),\n",
    "                                                  class_mode = 'sparse',\n",
    "                                                  batch_size = 64)\n",
    "\n",
    "# test data\n",
    "test_datasetRes = test_datagen.flow_from_directory(directory = './Desktop/CTscns2C/test',\n",
    "                                                  target_size = (224,224),\n",
    "                                                  class_mode = 'sparse',\n",
    "                                                  batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b7eaae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Cancerous\",\n",
    "\"normal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7def739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 18:18:40.799823: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# loading in the base model\n",
    "IMG_SHAPE = (224,224, 3)\n",
    "base_model_ResBinary = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000e1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezing layers\n",
    "base_model_ResBinary.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47c2dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining metrics\n",
    "def f1_score(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "METRICS = [\n",
    "      'accuracy',f1_score\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24b7b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding additional layers\n",
    "modelResBinary = Sequential([\n",
    "    base_model_ResBinary,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# compiling the model, adam optimizer\n",
    "modelResBinary.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[METRICS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f3bb73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,112,770\n",
      "Trainable params: 525,058\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelResBinary.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a021962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback to prevent overfitting\n",
    "early_stopping = EarlyStopping(min_delta=0.01, patience=2, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d6a9fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "9/9 [==============================] - 31s 3s/step - loss: 0.6784 - accuracy: 0.6543 - f1_score: 0.6203 - val_loss: 0.4762 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n",
      "Epoch 2/15\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.3703 - accuracy: 0.8836 - f1_score: 0.6179 - val_loss: 0.3605 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n",
      "Epoch 3/15\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.2797 - accuracy: 0.9189 - f1_score: 0.6178 - val_loss: 0.2775 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n",
      "Epoch 4/15\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.2167 - accuracy: 0.9400 - f1_score: 0.6177 - val_loss: 0.1077 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n",
      "Epoch 5/15\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.1843 - accuracy: 0.9489 - f1_score: 0.6195 - val_loss: 0.1138 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n",
      "Epoch 6/15\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.1613 - accuracy: 0.9577 - f1_score: 0.6197 - val_loss: 0.0612 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n",
      "Epoch 7/15\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.1638 - accuracy: 0.9524 - f1_score: 0.6191 - val_loss: 0.1242 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n",
      "Epoch 8/15\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.1312 - accuracy: 0.9594 - f1_score: 0.6196 - val_loss: 0.0477 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n",
      "Epoch 9/15\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.1616 - accuracy: 0.9418 - f1_score: 0.6167 - val_loss: 0.0270 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n",
      "Epoch 10/15\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.1547 - accuracy: 0.9559 - f1_score: 0.6166 - val_loss: 0.0266 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n",
      "Epoch 11/15\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.1634 - accuracy: 0.9489 - f1_score: 0.6189 - val_loss: 0.1043 - val_accuracy: 1.0000 - val_f1_score: 0.1481\n"
     ]
    }
   ],
   "source": [
    "# training model over training data\n",
    "historyResBinary = modelResBinary.fit(train_datasetRes, epochs=15, batch_size=36, validation_data=valid_datasetRes, \n",
    "                             callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf75dc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 237ms/step - loss: 0.2170 - accuracy: 0.9265 - f1_score: 0.5333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21695712208747864, 0.9264705777168274, 0.5333333015441895]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating over the test data\n",
    "\n",
    "modelResBinary.evaluate(test_datasetRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e5018f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# test over unseen data as well as holdout\n",
    "# testing against unseen data\n",
    "\n",
    "unseentest_dataset = test_datagen.flow_from_directory(directory = './Desktop/CTbenignremoved/test',\n",
    "                                                  target_size = (224,224),\n",
    "                                                  class_mode = 'sparse',\n",
    "                                                  batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a68fc912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 5s 2s/step - loss: 3.1718 - accuracy: 0.4600 - f1_score: 0.6044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.1718082427978516, 0.46000000834465027, 0.6043955683708191]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelResBinary.evaluate(unseentest_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
